import dotenv from "dotenv";
dotenv.config();

const OPENAI_API_KEY = process.env.OPENAI_API_KEY;
const MODEL = String(process.env.OPENAI_MODEL || "").trim();
const MAX_TOKENS = Number(process.env.OPENAI_MAX_TOKENS || 300);
const OPENAI_TIMEOUT_MS = Number(process.env.OPENAI_TIMEOUT_MS || 20000);
const TOKENS_PER_COMMENT = Number(process.env.OPENAI_TOKENS_PER_COMMENT || 100);
const MAX_TOKENS_CAP = Number(process.env.OPENAI_MAX_TOKENS_CAP || 900);

async function openAIChatCompletions(requestBody) {
  const controller = new AbortController();
  const timeoutId = setTimeout(() => controller.abort(), OPENAI_TIMEOUT_MS);
  try {
    const response = await fetch("https://api.openai.com/v1/chat/completions", {
      method: "POST",
      headers: {
        Authorization: `Bearer ${OPENAI_API_KEY}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify(requestBody),
      signal: controller.signal,
    });
    let result = null;
    try {
      result = await response.json();
    } catch {
      result = null;
    }
    return { response, result };
  } catch (error) {
    const timedOut =
      error?.name === "AbortError" ||
      String(error?.message || "").toLowerCase().includes("aborted");
    if (timedOut) {
      throw new Error(`OpenAI request timed out after ${OPENAI_TIMEOUT_MS}ms`);
    }
    throw error;
  } finally {
    clearTimeout(timeoutId);
  }
}

function extractJsonCandidates(text) {
  const candidates = [];
  if (typeof text !== "string") return candidates;
  const trimmed = text.trim();
  if (!trimmed) return candidates;

  // Candidate 1: raw content as-is.
  candidates.push(trimmed);

  // Candidate 2: fenced code blocks (```json ... ``` or ``` ... ```).
  const fencedMatches = trimmed.match(/```(?:json)?\s*([\s\S]*?)```/gi);
  if (Array.isArray(fencedMatches)) {
    for (const block of fencedMatches) {
      const inner = block
        .replace(/^```(?:json)?\s*/i, "")
        .replace(/```$/i, "")
        .trim();
      if (inner) candidates.push(inner);
    }
  }

  // Candidate 3: first balanced JSON object in the text.
  let depth = 0;
  let start = -1;
  let inString = false;
  let escaped = false;
  for (let i = 0; i < trimmed.length; i++) {
    const ch = trimmed[i];
    if (inString) {
      if (escaped) {
        escaped = false;
      } else if (ch === "\\") {
        escaped = true;
      } else if (ch === '"') {
        inString = false;
      }
      continue;
    }
    if (ch === '"') {
      inString = true;
      continue;
    }
    if (ch === "{") {
      if (depth === 0) start = i;
      depth++;
    } else if (ch === "}") {
      depth--;
      if (depth === 0 && start >= 0) {
        const maybeJson = trimmed.slice(start, i + 1).trim();
        if (maybeJson) {
          candidates.push(maybeJson);
        }
        break;
      }
    }
  }

  // De-duplicate while preserving order.
  return [...new Set(candidates)];
}

export async function OpenAIHealthCheck() {
  const startedAt = Date.now();
  const checked_at = new Date().toISOString();

  if (!OPENAI_API_KEY) {
    return {
      live: false,
      connected: false,
      configured: false,
      model: MODEL,
      latency_ms: 0,
      checked_at,
      detail: "OPENAI_API_KEY is not configured",
      code: "missing_api_key",
    };
  }

  if (!MODEL) {
    return {
      live: false,
      connected: false,
      configured: false,
      model: null,
      latency_ms: 0,
      checked_at,
      detail: "OPENAI_MODEL is not configured",
      code: "missing_model",
    };
  }

  const controller = new AbortController();
  const timeoutId = setTimeout(() => controller.abort(), OPENAI_TIMEOUT_MS);

  try {
    const response = await fetch(
      "https://api.openai.com/v1/models",
      {
        method: "GET",
        headers: {
          Authorization: `Bearer ${OPENAI_API_KEY}`,
          "Content-Type": "application/json",
        },
        signal: controller.signal,
      },
    );

    const latency_ms = Date.now() - startedAt;
    let body = null;
    try {
      body = await response.json();
    } catch {
      body = null;
    }

    if (response.ok) {
      const models = Array.isArray(body?.data) ? body.data : [];
      const modelAvailable = models.some((m) => m?.id === MODEL);
      return {
        live: modelAvailable,
        connected: true,
        configured: true,
        model: MODEL,
        latency_ms,
        checked_at,
        detail: modelAvailable
          ? "OpenAI API reachable and model available"
          : `OpenAI API reachable, but model "${MODEL}" is not listed`,
        code: modelAvailable ? "ok" : "model_unavailable",
        model_available: modelAvailable,
      };
    }

    if (response.status === 404) {
      return {
        live: false,
        connected: true,
        configured: true,
        model: MODEL,
        latency_ms,
        checked_at,
        detail: `Model "${MODEL}" not found or unavailable`,
        code: "model_unavailable",
        http_status: response.status,
        provider_error: body?.error?.message || null,
      };
    }

    return {
      live: false,
      connected: true,
      configured: true,
      model: MODEL,
      latency_ms,
      checked_at,
      detail: "OpenAI API returned an error",
      code: "provider_error",
      http_status: response.status,
      provider_error: body?.error?.message || null,
    };
  } catch (error) {
    const latency_ms = Date.now() - startedAt;
    const timedOut =
      error?.name === "AbortError" || String(error?.message || "").includes("aborted");

    return {
      live: false,
      connected: false,
      configured: true,
      model: MODEL,
      latency_ms,
      checked_at,
      detail: timedOut ? "OpenAI health check timed out" : "OpenAI health check failed",
      code: timedOut ? "timeout" : "network_error",
      provider_error: error?.message || null,
    };
  } finally {
    clearTimeout(timeoutId);
  }
}

export async function OpenAISendBatch(batch, codebook) {
  if (!MODEL) {
    throw new Error("OPENAI_MODEL is not configured.");
  }
  const prompt = buildRecodingPrompt(batch, codebook);
  const requestedMaxTokens = Math.min(
    MAX_TOKENS_CAP,
    Math.max(MAX_TOKENS, 120 + batch.length * TOKENS_PER_COMMENT),
  );

  const requestBody = {
    model: MODEL,
    messages: [
      {
        role: "user",
        content: prompt,
      },
    ],
    max_completion_tokens: requestedMaxTokens,
    response_format: { type: "json_object" },
  };

  const initialCall = await openAIChatCompletions(requestBody);
  const response = initialCall.response;
  let result = initialCall.result;
  let responseOk = response.ok;

  // Compatibility fallback for providers/models that only accept max_tokens.
  if (
    !response.ok &&
    String(result?.error?.message || "").includes("max_completion_tokens")
  ) {
    const fallbackResponse = await openAIChatCompletions({
      model: MODEL,
      messages: requestBody.messages,
      max_tokens: requestedMaxTokens,
      response_format: { type: "json_object" },
    });
    result = fallbackResponse.result;

    responseOk = fallbackResponse.response.ok;

    if (!responseOk) {
      const message =
        result?.error?.message ||
        "OpenAI processing failed. Please try again later.";
      throw new Error(message);
    }
  }

  // Some providers/models may not support response_format on chat completions.
  if (
    !responseOk &&
    String(result?.error?.message || "").toLowerCase().includes("response_format")
  ) {
    const fallbackNoSchema = await openAIChatCompletions({
      model: MODEL,
      messages: requestBody.messages,
      max_completion_tokens: requestedMaxTokens,
    });
    result = fallbackNoSchema.result;
    responseOk = fallbackNoSchema.response.ok;
  }

  if (!responseOk) {
    // ðŸ”‘ Fail fast, but clearly
    const message =
      result?.error?.message ||
      "OpenAI processing failed. Please try again later.";

    throw new Error(message);
  }

  const rawContent = result?.choices?.[0]?.message?.content;
  let outputText = "";

  if (typeof rawContent === "string") {
    outputText = rawContent.trim();
  } else if (Array.isArray(rawContent)) {
    outputText = rawContent
      .map((part) => (typeof part?.text === "string" ? part.text : ""))
      .join("")
      .trim();
  }

  if (!outputText) {
    // Retry once with a compact prompt to avoid empty/truncated completions.
    const compactPrompt = buildRecodingPromptCompact(batch, codebook);
    const retryResponse = await openAIChatCompletions({
      model: MODEL,
      messages: [{ role: "user", content: compactPrompt }],
      max_completion_tokens: requestedMaxTokens,
    });
    if (retryResponse.response.ok) {
      const retryResult = retryResponse.result;
      const retryText = String(retryResult?.choices?.[0]?.message?.content || "").trim();
      if (retryText) {
        const retryCandidates = extractJsonCandidates(retryText);
        for (const candidate of retryCandidates) {
          try {
            return JSON.parse(candidate);
          } catch {
            // try next
          }
        }
      }
    }
    console.warn("âš ï¸ OpenAI returned empty output. Falling back to codebook-only themes.");
    return null;
  }

  const jsonCandidates = extractJsonCandidates(outputText);
  for (const candidate of jsonCandidates) {
    try {
      return JSON.parse(candidate);
    } catch {
      // Try next candidate
    }
  }

  console.warn("âš ï¸ OpenAI returned unparsable JSON payload. Falling back to codebook-only themes.");
  return null;
}

function buildRecodingPrompt(batch, codebook) {
  let prompt = `
You are a qualitative researcher recoding student comments using a standardized codebook.

Use ONLY the categories in the codebook. Assign 1â€“3 categories per comment.
Return STRICT JSON only.

CODEBOOK:
${JSON.stringify(codebook, null, 2)}

FORMAT:
{
  "Comment 1": ["category"],
  "Comment 2": []
}

Now code the following comments:
`;

  batch.forEach((row, i) => {
    prompt += `\nComment ${i + 1}:\n${row.text}\n`;
  });

  return prompt;
}

function buildRecodingPromptCompact(batch, codebook) {
  const categories = Array.isArray(codebook)
    ? codebook.map((c) => c?.category).filter(Boolean)
    : [];
  let prompt = `Return STRICT JSON only.\nUse only these categories: ${JSON.stringify(categories)}\nAssign 1-3 categories per comment.\nFormat: {"Comment 1":["category"],"Comment 2":[]}\n`;
  batch.forEach((row, i) => {
    prompt += `\nComment ${i + 1}: ${String(row.text || "").slice(0, 900)}\n`;
  });
  return prompt;
}
